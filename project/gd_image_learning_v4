
import torch # í…ì„œ ìƒì„±, ì—°ì‚°
import torch.nn as nn # ì‹ ê²½ë§ì˜ ì¸µ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.optim as optim # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
from torch.utils.data import DataLoader, Dataset, random_split, Subset
from torchvision import transforms, models, datasets
import numpy as np
from google.colab import drive
import os
from PIL import Image
import datetime
import glob
import random
from torch.utils.data import DataLoader, TensorDataset # TensorDataset -> X, yë¥¼ í…ì„œí˜•íƒœë¡œ ë‚˜ëˆ„ê¸°
import pandas as pd
from sklearn.preprocessing import LabelEncoder # label ì „ì²˜ë¦¬
from tqdm.notebook import tqdm # ì—í¬í¬ ìƒí™© í™•ì¸í•˜ê¸°
from sklearn.model_selection import train_test_split

drive.mount('/content/drive')
data_dir = '/content/drive/MyDrive/Food_allergy/image'
checkpoint_path = '/content/drive/MyDrive/Food_allergy/checkpoint.pth'
log_path = '/content/drive/MyDrive/Food_allergy/epoch_accuracy_log.txt'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ---------------------------------------------------
# 1. Dataset í´ë˜ìŠ¤ ì •ì˜ (ê°€ì¥ í•˜ìœ„ í´ë” = í´ë˜ìŠ¤)
# ---------------------------------------------------
class CustomImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.samples = []
        self.classes = []
        self.class_to_idx = {}
        self.transform = transform

        # ê°€ì¥ í•˜ìœ„ í´ë”(ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” í´ë”)ë¥¼ í´ë˜ìŠ¤ í´ë”ë¡œ ì¸ì‹
        for root, dirs, files in os.walk(root_dir):
            if len(files) > 0:  # ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” í´ë”ë¼ë©´
                class_name = os.path.basename(root)
                if class_name not in self.class_to_idx:
                    self.class_to_idx[class_name] = len(self.classes)
                    self.classes.append(class_name)

                for file in files:
                    if file.lower().endswith(('png','jpg','jpeg','bmp')):
                        self.samples.append((os.path.join(root, file), self.class_to_idx[class_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, label

# ---------------------------------------------------
# 2. ë°ì´í„° ë¡œë“œ ë° train/test ë¶„í• 
# ---------------------------------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

dataset = CustomImageDataset(data_dir, transform=transform)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

num_classes = len(dataset.classes)

# ---------------------------------------------------
# 3. ëª¨ë¸ ì •ì˜
# ---------------------------------------------------
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ---------------------------------------------------
# 4. ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
# ---------------------------------------------------
start_epoch = 0
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)

    if "model_state_dict" in checkpoint:   # ì œ ì½”ë“œ í¬ë§·
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ (epoch {start_epoch}ë¶€í„° ì‹œì‘)")

    elif "model_state" in checkpoint:      # ê¸°ì¡´ì— ì €ì¥í•œ í¬ë§·
        model.load_state_dict(checkpoint['model_state'])
        optimizer.load_state_dict(checkpoint['optimizer_state'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"âœ… (êµ¬ë²„ì „) ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ (epoch {start_epoch}ë¶€í„° ì‹œì‘)")

    else:  # ê·¸ëƒ¥ state_dictë§Œ ì €ì¥ëœ ê²½ìš°
        model.load_state_dict(checkpoint)
        print("âš ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. epoch=0ë¶€í„° ì‹œì‘")

# ---------------------------------------------------
# 5. í•™ìŠµ í•¨ìˆ˜
# ---------------------------------------------------
def train(model, loader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(loader):
        inputs, labels = inputs.to(device), labels.to(device)

        # ìˆœì „íŒŒ
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # ì—­ì „íŒŒ
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        if (i+1) % 10 == 0:
            print(f"[Epoch {epoch+1}] Step {i+1}/{len(loader)}, Loss: {loss.item():.4f}")

    return running_loss / len(loader)

# ---------------------------------------------------
# 6. í‰ê°€ í•¨ìˆ˜ (í´ë˜ìŠ¤ë³„ ì •í™•ë„)
# ---------------------------------------------------
def evaluate(model, loader, classes, epoch, log_file):
    model.eval()
    correct = 0
    total = 0
    class_correct = [0]*len(classes)
    class_total = [0]*len(classes)

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            correct += (preds == labels).sum().item()
            total += labels.size(0)

            for i in range(len(labels)):
                label = labels[i]
                class_total[label] += 1
                if preds[i] == label:
                    class_correct[label] += 1

    overall_acc = 100 * correct / total
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"Epoch {epoch+1}:\n")
        for i, classname in enumerate(classes):
            acc = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
            f.write(f"  {classname}: {acc:.2f}%\n")
        f.write(f"  Overall: {overall_acc:.2f}%\n\n")

    print(f"âœ… Epoch {epoch+1} í…ŒìŠ¤íŠ¸ ì •í™•ë„: {overall_acc:.2f}%")
    return overall_acc

# ---------------------------------------------------
# 7. í•™ìŠµ ë£¨í”„
# ---------------------------------------------------
num_epochs = 10
for epoch in range(start_epoch, num_epochs):
    loss = train(model, train_loader, optimizer, criterion, epoch)
    acc = evaluate(model, test_loader, dataset.classes, epoch, log_path)

    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, checkpoint_path)

    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ (epoch {epoch+1})")

print("ğŸ‰ í•™ìŠµ ì™„ë£Œ")

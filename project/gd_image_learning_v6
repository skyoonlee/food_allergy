import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms, models
from PIL import Image
from tqdm.notebook import tqdm
import os

# ------------------ μ„¤μ • ------------------
data_dir = '/content/drive/MyDrive/Food_allergy/image'
checkpoint_path = '/content/drive/MyDrive/Food_allergy/checkpoint.pth'
log_path = '/content/drive/MyDrive/Food_allergy/epoch_accuracy_log.txt'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_epochs = 20
batch_size = 64
lr = 1e-4

# ------------------ Dataset ------------------
class CustomImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.samples = []
        self.classes = []
        self.class_to_idx = {}
        self.transform = transform

        for root, dirs, files in os.walk(root_dir):
            if len(files) > 0:
                class_name = os.path.basename(root)
                if class_name not in self.class_to_idx:
                    self.class_to_idx[class_name] = len(self.classes)
                    self.classes.append(class_name)
                for file in files:
                    if file.lower().endswith(('png','jpg','jpeg','bmp')):
                        self.samples.append((os.path.join(root, file), self.class_to_idx[class_name]))

        if len(self.samples) == 0:
            raise ValueError(f"λ°μ΄ν„°κ°€ μ—†μµλ‹λ‹¤! {root_dir} κ²½λ΅λ¥Ό ν™•μΈν•μ„Έμ”.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, label

# ------------------ Transform & DataLoader ------------------
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

dataset = CustomImageDataset(data_dir, transform=transform)

train_size = int(0.7 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

if len(train_loader) == 0 or len(test_loader) == 0:
    raise ValueError("ν•™μµ λλ” ν…μ¤νΈ λ°μ΄ν„°κ°€ μ—†μµλ‹λ‹¤. κ²½λ΅μ™€ νμΌ ν™•μΈ ν•„μ”!")

num_classes = len(dataset.classes)
print(f"β… ν΄λμ¤ λ§¤ν•‘: {dataset.class_to_idx}")
print(f"β… ν•™μµ μƒν” μ: {len(train_dataset)}, ν…μ¤νΈ μƒν” μ: {len(test_dataset)}")

# ------------------ λ¨λΈ ------------------
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# ------------------ μ²΄ν¬ν¬μΈνΈ λ¶λ¬μ¤κΈ° ------------------
start_epoch = 0
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    if "model_state_dict" in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"β… μ²΄ν¬ν¬μΈνΈ λ¶λ¬μ¤κΈ° μ„±κ³µ (epoch {start_epoch}λ¶€ν„° μ‹μ‘)")
    else:
        model.load_state_dict(checkpoint)
        print("β οΈ λ¨λΈ νλΌλ―Έν„°λ§ λ¶λ¬μ™”μµλ‹λ‹¤. epoch=0λ¶€ν„° μ‹μ‘")

# ------------------ ν•™μµ ν•¨μ ------------------
def train_epoch(model, loader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    pbar = tqdm(enumerate(loader), total=len(loader), desc=f"Epoch {epoch+1}")
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        pbar.set_postfix(loss=loss.item())
    return running_loss / len(loader)

# ------------------ ν‰κ°€ ν•¨μ ------------------
def evaluate(model, loader, classes):
    model.eval()
    correct = 0
    total = 0
    class_correct = [0]*len(classes)
    class_total = [0]*len(classes)
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            for i in range(len(labels)):
                label = labels[i]
                class_total[label] += 1
                if preds[i] == label:
                    class_correct[label] += 1
    overall_acc = 100 * correct / total
    return overall_acc, class_correct, class_total

# ------------------ ν•™μµ λ£¨ν”„ ------------------
for epoch in range(start_epoch, num_epochs):
    avg_loss = train_epoch(model, train_loader, optimizer, criterion, epoch)
    overall_acc, class_correct, class_total = evaluate(model, test_loader, dataset.classes)

    print(f"[Epoch {epoch+1}] ν‰κ·  μ†μ‹¤: {avg_loss:.4f}, ν…μ¤νΈ μ •ν™•λ„: {overall_acc:.2f}%")
    for i, cls in enumerate(dataset.classes):
        acc = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        print(f"  {cls}: {acc:.2f}%")

    # μ²΄ν¬ν¬μΈνΈ μ €μ¥
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'class_to_idx': dataset.class_to_idx
    }, checkpoint_path)
    print(f"π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ™„λ£ (epoch {epoch+1})\n")

# ------------------ μµμΆ… λ¨λΈ μ €μ¥ ------------------
final_model_path = '/content/drive/MyDrive/Food_allergy/final_model.pt'
torch.save(model.state_dict(), final_model_path)
print(f"π‰ μµμΆ… λ¨λΈ μ €μ¥ μ™„λ£: {final_model_path}")

# ------------------ ν΄λμ¤ μμ„ μ €μ¥ ------------------
class_txt_path = '/content/drive/MyDrive/Food_allergy/class_to_idx.txt'
with open(class_txt_path, 'w', encoding='utf-8') as f:
    for cls, idx in dataset.class_to_idx.items():
        f.write(f"{cls}\t{idx}\n")
print(f"π‰ ν΄λμ¤ λ§¤ν•‘ νμΌ μ €μ¥ μ™„λ£: {class_txt_path}")

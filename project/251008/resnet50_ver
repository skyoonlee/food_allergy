from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms, models
from PIL import Image
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import os

drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/Food_allergy/image'
base_save_dir = '/content/drive/MyDrive/Food_allergy/results'
os.makedirs(base_save_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í•™ìŠµ ì„¤ì •
num_epochs = 60
batch_size = 32
lr = 0.0005
test_ratio = 0.25  # í…ŒìŠ¤íŠ¸ì…‹ ë¹„ìœ¨

# ------------------ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ------------------
class CustomImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.samples = []
        self.classes = []
        self.class_to_idx = {}
        self.transform = transform

        for root, dirs, files in os.walk(root_dir):
            dirs.sort()
            if len(files) > 0:
                class_name = os.path.basename(root)
                if class_name not in self.class_to_idx:
                    self.class_to_idx[class_name] = len(self.classes)
                    self.classes.append(class_name)
                for file in files:
                    if file.lower().endswith(('png','jpg','jpeg','bmp')):
                        self.samples.append((os.path.join(root, file), self.class_to_idx[class_name]))

        if len(self.samples) == 0:
            raise ValueError(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! {root_dir} ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, label

# ------------------ Transform & DataLoader ------------------
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

dataset = CustomImageDataset(data_dir, transform=transform)
train_size = int((1-test_ratio) * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

num_classes = len(dataset.classes)
print(f"âœ… í´ë˜ìŠ¤ ë§¤í•‘: {dataset.class_to_idx}")
print(f"âœ… í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}, í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")

# ------------------ âœ… ëª¨ë¸: ResNet50 ------------------
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=5
)

# ------------------ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ------------------
checkpoint_path = os.path.join(base_save_dir, 'best_checkpoint_resnet50.pth')

# ------------------ í•™ìŠµ/í‰ê°€ í•¨ìˆ˜ ------------------
def train_epoch(model, loader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    pbar = tqdm(enumerate(loader), total=len(loader), desc=f"Epoch {epoch+1}")
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        pbar.set_postfix(loss=loss.item())
    return running_loss / len(loader)

def evaluate(model, loader, classes):
    model.eval()
    correct = 0
    total = 0
    class_correct = [0]*len(classes)
    class_total = [0]*len(classes)
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            for i in range(len(labels)):
                label = labels[i]
                class_total[label] += 1
                if preds[i] == label:
                    class_correct[label] += 1
    overall_acc = 100 * correct / total
    return overall_acc, class_correct, class_total

# ------------------ í•™ìŠµ ë£¨í”„ ------------------
loss_list, acc_list = [], []
best_acc = 0

# ê¸°ì¡´ checkpoint ë¶ˆëŸ¬ì˜¤ê¸° (ìˆìœ¼ë©´ ì´ì–´ì„œ í•™ìŠµ)
start_epoch = 0
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    best_acc = checkpoint['best_acc']
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. Epoch {start_epoch}, Best Accuracy {best_acc:.2f}%")

for epoch in range(start_epoch, num_epochs):
    avg_loss = train_epoch(model, train_loader, optimizer, criterion, epoch)
    overall_acc, class_correct, class_total = evaluate(model, test_loader, dataset.classes)

    loss_list.append(avg_loss)
    acc_list.append(overall_acc)

    if overall_acc > best_acc:
        best_acc = overall_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_acc': best_acc,
            'class_to_idx': dataset.class_to_idx
        }, checkpoint_path)
        print(f"ğŸ’¾ ìƒˆë¡œìš´ ìµœê³  ì •í™•ë„ ì €ì¥: {best_acc:.2f}% (Epoch {epoch+1})")

    print(f"[Epoch {epoch+1}] í‰ê·  ì†ì‹¤: {avg_loss:.4f}, í…ŒìŠ¤íŠ¸ ì •í™•ë„: {overall_acc:.2f}%")
    scheduler.step(overall_acc)

# ------------------ ê²°ê³¼ í´ë” ìƒì„± ------------------
folder_name = f"resnet50_e{num_epochs}_b{batch_size}_lr{str(lr).replace('.', '')}_acc{best_acc:.2f}"
save_dir = os.path.join(base_save_dir, folder_name)
os.makedirs(save_dir, exist_ok=True)

# ------------------ ê·¸ë˜í”„ ì €ì¥ ------------------
plt.figure(figsize=(8,5))
plt.plot(range(1, len(loss_list)+1), loss_list, label='Loss', marker='o')
plt.plot(range(1, len(acc_list)+1), acc_list, label='Accuracy', marker='s')
plt.title('Training Progress (ResNet50)')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
graph_path = os.path.join(save_dir, f"training_graph_{folder_name}.png")
plt.savefig(graph_path)
plt.close()
print(f"ğŸ“Š í•™ìŠµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {graph_path}")

# ------------------ ë¡œê·¸ ì €ì¥ ------------------
log_path = os.path.join(save_dir, f"epoch_accuracy_log_{folder_name}.txt")
class_log_path = os.path.join(save_dir, f"class_accuracy_log_{folder_name}.txt")

with open(log_path, 'w', encoding='utf-8') as f:
    f.write("Epoch\tLoss\tAccuracy\n")
    for i, (l, a) in enumerate(zip(loss_list, acc_list)):
        f.write(f"{i+1}\t{l:.4f}\t{a:.2f}\n")

with open(class_log_path, 'w', encoding='utf-8') as f:
    f.write("ìµœì¢… Epoch í´ë˜ìŠ¤ë³„ ì •í™•ë„\n\n")
    _, class_correct, class_total = evaluate(model, test_loader, dataset.classes)
    for i, cls in enumerate(dataset.classes):
        acc = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0
        f.write(f"{cls}: {acc:.2f}%\n")

# ------------------ ëª¨ë¸ ë° í´ë˜ìŠ¤ ì €ì¥ ------------------
final_model_path = os.path.join(save_dir, f"final_model_{folder_name}.pt")
torch.save(model.state_dict(), final_model_path)

class_txt_path = os.path.join(save_dir, f"class_to_idx_{folder_name}.txt")
with open(class_txt_path, 'w', encoding='utf-8') as f:
    for cls, idx in dataset.class_to_idx.items():
        f.write(f"{cls}\t{idx}\n")

print(f"\n ì €ì¥ ì™„ë£Œ:")
print(f" í´ë”: {save_dir}")
print(f" ëª¨ë¸: {final_model_path}")
print(f" ê·¸ë˜í”„: {graph_path}")
print(f" ë¡œê·¸: {log_path}")
print(f" í´ë˜ìŠ¤ë³„ ì •í™•ë„: {class_log_path}")
